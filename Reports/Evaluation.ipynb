{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "plt.rcParams['figure.figsize'] = [10, 5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Continual Learning Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Because of a mistake in my implementation\n",
    "# [\"no_of_test\"] cannot be used but it can be calculated by [\"no_of_correct_prediction\"]/[\"accuracy\"]\n",
    "# but it cannot be calculated when [\"accuracy\"] == 0\n",
    "\n",
    "# ((raw[\"no_of_correct_prediction\"]/ raw[\"accuracy\"]).apply(np.ceil))\n",
    "\n",
    "# the mistake have been fixed now but the data have not updated\n",
    "\n",
    "def calculateContinualMetircs(raw):\n",
    "    task_order = raw[\"task_order\"].unique()\n",
    "    method = raw[\"method\"].unique()\n",
    "    print(task_order, method)\n",
    "    \n",
    "    all_MBase = {k:[] for k in method}\n",
    "    all_Mnew  = {k:[] for k in method}\n",
    "    all_Mnow  = {k:[] for k in method}\n",
    "\n",
    "    for t in task_order:\n",
    "        rows = raw[raw[\"task_order\"]==t]\n",
    "        offline = rows[rows[\"method\"]==\"offline\"]\n",
    "\n",
    "\n",
    "        for m in method:\n",
    "            if m==\"offline\":\n",
    "                continue\n",
    "\n",
    "            target = rows[rows[\"method\"]==m]\n",
    "\n",
    "            # calculate m_base\n",
    "            _ideal = offline[offline[\"task_index\"]==1][\"accuracy\"]\n",
    "            _m = target[target[\"task_index\"]==1][[\"accuracy\", \"no_of_test\", \"no_of_correct_prediction\"]]\n",
    "\n",
    "            _N = len(_m)\n",
    "            _m = (_m[\"accuracy\"]/float(_ideal)).sum()\n",
    "            Mbase = float(_m/_N)\n",
    "\n",
    "            all_MBase[m].append(Mbase)\n",
    "\n",
    "\n",
    "            _sum = 0.0\n",
    "            train_session = target[\"train_session\"].unique()\n",
    "            for s in train_session:\n",
    "                s = int(s)\n",
    "                _ideal = offline[offline[\"task_index\"]==s][\"accuracy\"]\n",
    "\n",
    "                _m = target[target[\"train_session\"]==str(s)]\n",
    "                _m = _m[_m[\"task_index\"]==s][\"accuracy\"]\n",
    "\n",
    "                assert len(_m)==1\n",
    "\n",
    "                _sum += float(_m)/float(_ideal)\n",
    "\n",
    "            Mnew = _sum/len(train_session)\n",
    "            all_Mnew[m].append(Mnew)\n",
    "\n",
    "\n",
    "            _sum = 0.0\n",
    "            task_index = target[\"task_index\"].unique()\n",
    "            _m = target[target[\"train_session\"]==str(len(task_index))]\n",
    "            for t in task_index:\n",
    "                t = int(t)\n",
    "                _ideal = offline[offline[\"task_index\"]==t][\"accuracy\"]\n",
    "                _m1 = _m[_m[\"task_index\"]==t][\"accuracy\"]\n",
    "\n",
    "                assert len(_m1)==1\n",
    "\n",
    "                _sum += float(_m1)/float(_ideal)\n",
    "\n",
    "            Mnow = _sum/len(train_session)\n",
    "            all_Mnow[m].append(Mnow)\n",
    "            \n",
    "    return all_MBase, all_Mnew, all_Mnow\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import stats\n",
    "\n",
    "def printCLMetrics(all_MBase, all_Mnew, all_Mnow):\n",
    "    def p(metric, name):\n",
    "        \n",
    "        print(\"Metric: \", name)\n",
    "        for m in metric:\n",
    "            avg = np.mean(metric[m])\n",
    "            err = stats.sem(metric[m])\n",
    "            print(\"{0} {1:.3f} {2:.3f}\".format(m, avg, err))\n",
    "        print(\"=====================\")\n",
    "        print(\"\")\n",
    "            \n",
    "            \n",
    "    p(all_MBase, \"M base\")\n",
    "    p(all_Mnew, \"M new\")\n",
    "    p(all_Mnow, \"M now\")\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1 2 3 4 5 6 7 8 9] ['offline' 'none' 'exact' 'mp-gan' 'mp-wgan' 'sg-cgan' 'sg-cwgan']\n",
      "\n",
      "Metric:  M base\n",
      "offline nan nan\n",
      "none 0.247 0.025\n",
      "exact 1.141 0.118\n",
      "mp-gan 0.687 0.077\n",
      "mp-wgan 0.701 0.108\n",
      "sg-cgan 0.592 0.063\n",
      "sg-cwgan 0.367 0.051\n",
      "=====================\n",
      "\n",
      "Metric:  M new\n",
      "offline nan nan\n",
      "none 1.258 0.073\n",
      "exact 1.237 0.074\n",
      "mp-gan 1.257 0.074\n",
      "mp-wgan 1.256 0.074\n",
      "sg-cgan 1.258 0.073\n",
      "sg-cwgan 1.259 0.073\n",
      "=====================\n",
      "\n",
      "Metric:  M now\n",
      "offline nan nan\n",
      "none 0.234 0.018\n",
      "exact 1.108 0.063\n",
      "mp-gan 0.803 0.061\n",
      "mp-wgan 0.823 0.078\n",
      "sg-cgan 0.652 0.038\n",
      "sg-cwgan 0.307 0.036\n",
      "=====================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Result from newsrc/result_iter1000-1000_h500-100_all/\n",
    "\n",
    "folder = \"newsrc/result_iter1000-1000_h500-100_all/\"\n",
    "raw = pd.read_csv(folder+\"results.txt\")\n",
    "raw.columns = [c.strip() for c in raw.columns]\n",
    "\n",
    "raw.head()\n",
    "b, n, nw = calculateContinualMetircs(raw)\n",
    "\n",
    "print(\"\")\n",
    "printCLMetrics(b, n, nw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R2_work_at_computer 0.994 0.003\n",
      "R2_sleep 0.984 0.000\n",
      "R1_work_at_computer 1.000 0.000\n",
      "R2_prepare_lunch 0.007 0.007\n",
      "R2_bed_to_toilet 0.762 0.060\n",
      "R1_work_at_dining_room_table 0.677 0.035\n",
      "R2_watch_TV 0.898 0.045\n",
      "R1_bed_to_toilet 0.943 0.057\n",
      "R1_sleep 1.000 0.000\n",
      "R2_prepare_dinner 0.986 0.008\n"
     ]
    }
   ],
   "source": [
    "# Result from newsrc/result_iter1000-1000_h500-100_all/\n",
    "\n",
    "folder = \"../Results/run_offline_acc/\"\n",
    "raw = pd.read_csv(folder+\"results.txt\")\n",
    "raw.columns = [c.strip() for c in raw.columns]\n",
    "\n",
    "fto = open(folder+\"task_orders.txt\")\n",
    "task_orders = [line.strip().split(\";\") for line in fto]\n",
    "\n",
    "def offlineAccuracy(raw, task_orders):\n",
    "    acc = {k:[] for k in task_orders[0]}\n",
    "    for i, order in enumerate(task_orders):\n",
    "        \n",
    "        m = raw[raw[\"task_order\"]==i]\n",
    "        for k, row in m.iterrows():\n",
    "            c = order[row[\"task_index\"]-1]\n",
    "            acc[c].append(row[\"accuracy\"])\n",
    "            \n",
    "    for m in acc:\n",
    "        avg = np.mean(acc[m])\n",
    "        err = stats.sem(acc[m])\n",
    "        print(\"{0} {1:.3f} {2:.3f}\".format(m, avg, err))\n",
    "\n",
    "offlineAccuracy(raw, task_orders)\n",
    "# print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1 2 3 4 5 6 7 8 9] ['offline' 'none' 'exact' 'mp-gan' 'mp-wgan' 'sg-cgan' 'sg-cwgan']\n",
      "\n",
      "Metric:  M base\n",
      "offline nan nan\n",
      "none 0.300 0.046\n",
      "exact 1.371 0.170\n",
      "mp-gan 0.903 0.080\n",
      "mp-wgan 0.924 0.094\n",
      "sg-cgan 0.884 0.111\n",
      "sg-cwgan 0.713 0.047\n",
      "=====================\n",
      "\n",
      "Metric:  M new\n",
      "offline nan nan\n",
      "none 1.222 0.032\n",
      "exact 1.201 0.029\n",
      "mp-gan 1.222 0.031\n",
      "mp-wgan 1.222 0.032\n",
      "sg-cgan 1.224 0.031\n",
      "sg-cwgan 1.223 0.031\n",
      "=====================\n",
      "\n",
      "Metric:  M now\n",
      "offline nan nan\n",
      "none 0.265 0.028\n",
      "exact 1.049 0.010\n",
      "mp-gan 0.776 0.033\n",
      "mp-wgan 0.801 0.034\n",
      "sg-cgan 0.725 0.041\n",
      "sg-cwgan 0.654 0.037\n",
      "=====================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Result from newsrc/result_iter1000-1000_h500-100_all/\n",
    "\n",
    "folder = \"newsrc/result_iter5000-1000_h500-100_all/\"\n",
    "raw = pd.read_csv(folder+\"results.txt\")\n",
    "raw.columns = [c.strip() for c in raw.columns]\n",
    "\n",
    "raw.head()\n",
    "b, n, nw = calculateContinualMetircs(raw)\n",
    "\n",
    "print(\"\")\n",
    "printCLMetrics(b, n, nw)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GAN Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model size\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'mp-gan': [1490061],\n",
       " 'mp-wgan': [1490061],\n",
       " 'sg-cgan': [151010],\n",
       " 'sg-cwgan': [151010]}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Model size\")\n",
    "{'mp-gan': [1490061], 'mp-wgan': [1490061], 'sg-cgan': [151010], 'sg-cwgan': [151010]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import stats\n",
    "\n",
    "def calculateGANMetircs(raw_gan, raw_solver):\n",
    "    \n",
    "    task_order = raw_gan[\"task_order\"].unique()\n",
    "    method = raw_gan[\"method\"].unique()\n",
    "    print(task_order, method)\n",
    "    \n",
    "    is_score = {k:[] for k in method}\n",
    "#     is_err = {k:[] for k in method}\n",
    "    mmd_score = {k:[] for k in method}\n",
    "    knn_acc = {k:[] for k in method}\n",
    "    knn_TPR = {k:[] for k in method}\n",
    "    knn_TNR = {k:[] for k in method}\n",
    "    offline_acc = {k:[] for k in method}\n",
    "    training_time = {k:[] for k in method}\n",
    "    \n",
    "    for t in task_order:\n",
    "        rows = raw_gan[raw_gan[\"task_order\"]==t]\n",
    "        for m in method:\n",
    "            _m = rows[rows[\"method\"]==m]\n",
    "            \n",
    "            _n = raw_solver[raw_solver[\"task_order\"]==t]\n",
    "            _n = pd.to_numeric(_n[_n[\"method\"]==m][\"generator_training_time\"]).sum()\n",
    "            \n",
    "            is_score[m].append(float(_m[\"is\"]))\n",
    "            mmd_score[m].append(float(_m[\"mmd\"]))\n",
    "            knn_acc[m].append(float(_m[\"knn_tp\"]+_m[\"knn_tn\"])/float(_m[\"knn_tp\"]+_m[\"knn_tn\"]+_m[\"knn_fp\"]+_m[\"knn_fn\"]))\n",
    "            knn_TPR[m].append(float(_m[\"knn_tp\"])/float(_m[\"knn_tp\"]+_m[\"knn_fn\"]))\n",
    "            knn_TNR[m].append(float(_m[\"knn_tn\"])/float(_m[\"knn_tn\"]+_m[\"knn_fp\"]))\n",
    "                              \n",
    "            offline_acc[m].append(float(_m[\"offline_acc_fake\"]))\n",
    "            training_time[m].append(_n)\n",
    "        \n",
    "    return is_score, mmd_score, knn_acc, knn_TPR, knn_TNR, offline_acc, training_time\n",
    "\n",
    "\n",
    "def printGANMetrics(metrics):\n",
    "    names = [\"IS Score\", \"MMD\", \"1-NN Acc\", \"1-NN TPR\", \"1-NN TNR\", \"Offline Acc\", \"Training Time\"]\n",
    "#     for i, metric in enumerate(metrics):\n",
    "#         print(\"Metric\", names[i])\n",
    "#         for m in metric:\n",
    "#             avg = np.mean(metric[m])\n",
    "#             err = stats.sem(metric[m])\n",
    "#             print(\"{0} {1:.3f} {2:.3f}\".format(m, avg, err))\n",
    "#         print(\"===================\")\n",
    "    \n",
    "    for m in metric[0]:\n",
    "#         for i, n in enumerate(names):\n",
    "#             metric = metrics[i]\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1 2 3 4 5 6 7 8 9] ['mp-gan' 'mp-wgan' 'sg-cgan' 'sg-cwgan']\n",
      "mp-gan\n",
      "IS Score 9.846 0.096\n",
      "MMD 0.109 0.005\n",
      "1-NN Acc 1.000 0.000\n",
      "1-NN TPR 1.000 0.000\n",
      "1-NN TNR 1.000 0.000\n",
      "Offline Acc 0.891 0.018\n",
      "Training Time 191.816 2.281\n",
      "===================\n",
      "mp-wgan\n",
      "IS Score 9.634 0.206\n",
      "MMD 0.108 0.006\n",
      "1-NN Acc 1.000 0.000\n",
      "1-NN TPR 1.000 0.000\n",
      "1-NN TNR 1.000 0.000\n",
      "Offline Acc 0.882 0.024\n",
      "Training Time 400.615 3.998\n",
      "===================\n",
      "sg-cgan\n",
      "IS Score 6.769 0.337\n",
      "MMD 0.266 0.010\n",
      "1-NN Acc 1.000 0.000\n",
      "1-NN TPR 1.000 0.000\n",
      "1-NN TNR 1.000 0.000\n",
      "Offline Acc 0.697 0.047\n",
      "Training Time 234.455 1.710\n",
      "===================\n",
      "sg-cwgan\n",
      "IS Score 1.819 0.293\n",
      "MMD 0.955 0.103\n",
      "1-NN Acc 1.000 0.000\n",
      "1-NN TPR 1.000 0.000\n",
      "1-NN TNR 1.000 0.000\n",
      "Offline Acc 0.256 0.119\n",
      "Training Time 372.805 3.642\n",
      "===================\n"
     ]
    }
   ],
   "source": [
    "folder = \"newsrc/result_iter1000-1000_h500-100_all/\"\n",
    "raw_gan = pd.read_csv(folder+\"gan_score.txt\")\n",
    "raw_gan.columns = [c.strip() for c in raw_gan.columns]\n",
    "\n",
    "raw_solver = pd.read_csv(folder+\"results.txt\")\n",
    "raw_solver.columns = [c.strip() for c in raw_solver.columns]\n",
    "\n",
    "m = calculateGANMetircs(raw_gan, raw_solver)\n",
    "\n",
    "def printGANMetrics(metrics):\n",
    "    names = [\"IS Score\", \"MMD\", \"1-NN Acc\", \"1-NN TPR\", \"1-NN TNR\", \"Offline Acc\", \"Training Time\"]\n",
    "#     for i, metric in enumerate(metrics):\n",
    "#         print(\"Metric\", names[i])\n",
    "#         for m in metric:\n",
    "#             avg = np.mean(metric[m])\n",
    "#             err = stats.sem(metric[m])\n",
    "#             print(\"{0} {1:.3f} {2:.3f}\".format(m, avg, err))\n",
    "#         print(\"===================\")\n",
    "    \n",
    "    for m in metrics[0]:\n",
    "        print(m)\n",
    "        for i, n in enumerate(names):\n",
    "            metric = metrics[i]\n",
    "            avg = np.mean(metric[m])\n",
    "            err = stats.sem(metric[m])\n",
    "            print(\"{0} {1:.3f} {2:.3f}\".format(n, avg, err))\n",
    "        print(\"===================\")\n",
    "        \n",
    "\n",
    "printGANMetrics(m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1 2 3 4 5 6 7 8 9] ['mp-gan' 'mp-wgan' 'sg-cgan' 'sg-cwgan']\n",
      "mp-gan\n",
      "IS Score 9.947 0.022\n",
      "MMD 0.082 0.005\n",
      "1-NN Acc 1.000 0.000\n",
      "1-NN TPR 1.000 0.000\n",
      "1-NN TNR 1.000 0.000\n",
      "Offline Acc 0.860 0.016\n",
      "Training Time 1137.738 6.542\n",
      "===================\n",
      "mp-wgan\n",
      "IS Score 9.869 0.101\n",
      "MMD 0.050 0.004\n",
      "1-NN Acc 1.000 0.000\n",
      "1-NN TPR 1.000 0.000\n",
      "1-NN TNR 1.000 0.000\n",
      "Offline Acc 0.866 0.010\n",
      "Training Time 3343.850 12.237\n",
      "===================\n",
      "sg-cgan\n",
      "IS Score 9.469 0.267\n",
      "MMD 0.151 0.012\n",
      "1-NN Acc 1.000 0.000\n",
      "1-NN TPR 1.000 0.000\n",
      "1-NN TNR 1.000 0.000\n",
      "Offline Acc 0.875 0.021\n",
      "Training Time 1434.469 1.498\n",
      "===================\n",
      "sg-cwgan\n",
      "IS Score 7.892 0.438\n",
      "MMD 0.253 0.010\n",
      "1-NN Acc 1.000 0.000\n",
      "1-NN TPR 1.000 0.000\n",
      "1-NN TNR 1.000 0.000\n",
      "Offline Acc 0.831 0.042\n",
      "Training Time 3081.804 14.606\n",
      "===================\n"
     ]
    }
   ],
   "source": [
    "folder = \"newsrc/result_iter5000-1000_h500-100_all/\"\n",
    "raw_gan = pd.read_csv(folder+\"gan_score.txt\")\n",
    "raw_gan.columns = [c.strip() for c in raw_gan.columns]\n",
    "\n",
    "raw_solver = pd.read_csv(folder+\"results.txt\")\n",
    "raw_solver.columns = [c.strip() for c in raw_solver.columns]\n",
    "\n",
    "m = calculateGANMetircs(raw_gan, raw_solver)\n",
    "printGANMetrics(m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
